{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Forecasting with LSTM Neural Networks\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Learn how to build a basic LSTM using Keras.\n",
    "- Train the LSTM model on unemployment data and forecast to future data.\n",
    "- Understand the different ways to fit the LSTM model for timeseries data.\n",
    "- Understand the \"stateful\" LSTM setup and how it compares to the standard fitting procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "\t- [A note on the lesson content](#a-note-on-the-lesson-content)\n",
    "- [Load the Keras modules](#load-the-keras-modules)\n",
    "- [Prepare the unemployment data for timeseries modeling](#prepare-the-unemployment-data-for-timeseries-modeling)\n",
    "\t- [Create the first-order differenced unemployment rate](#create-the-first-order-differenced-unemployment-rate)\n",
    "\t- [Normalize the differenced unemployement data with `MinMaxScaler`](#normalize-the-differenced-unemployement-data-with-minmaxscaler)\n",
    "\t- [Split the timeseries into 50% train/test splits](#split-the-timeseries-into--traintest-splits)\n",
    "- [Write a function to create the predictor and target data](#write-a-function-to-create-the-predictor-and-target-data)\n",
    "\t- [Create training and testing data for a lag of 1](#create-training-and-testing-data-for-a-lag-of-)\n",
    "- [Reshape the data to work with the LSTM](#reshape-the-data-to-work-with-the-lstm)\n",
    "- [Constructing the Keras model](#constructing-the-keras-model)\n",
    "\t- [Fit the LSTM model](#fit-the-lstm-model)\n",
    "\t- [Plot the original data, the training predictions and the testing predictions](#plot-the-original-data-the-training-predictions-and-the-testing-predictions)\n",
    "- [LSTM with multiple lags as predictors](#lstm-with-multiple-lags-as-predictors)\n",
    "- [Refraing the problem using the LSTM \"time steps\" dimension](#refraing-the-problem-using-the-lstm-time-steps-dimension)\n",
    "\t- [Rebuild and fit the LSTM model, and plot the predictions](#rebuild-and-fit-the-lstm-model-and-plot-the-predictions)\n",
    "- [\"Stateful\" LSTM models](#stateful-lstm-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Modeling timeseries and forecasting with neural networks is a growing trend. The Long Short Term Memory (LSTM) recurrent neural network architecture is a popular choice when \"context\" or memory across time is a desired capability of the model.\n",
    "\n",
    "In this walkthrough/codealong lecture we will be building an LSTM using the Keras framework to forecast stock market timeseries data. \n",
    "\n",
    "<a id=\"a-note-on-the-lesson-content\"></a>\n",
    "### A note on the lesson content\n",
    "\n",
    "This codealong focuses primarily on the Keras implementation and application of LSTM neural networks. This lecture does not cover:\n",
    "- The theory behind recurrent neural networks.\n",
    "- The mathematics or theory behind the architecture of LSTM networks.\n",
    "\n",
    "There are a variety of great resources to dive deeper into LSTM networks:\n",
    "- [A beginners guide to recurrent networks and LSTMs](http://deeplearning4j.org/lstm.html#a-beginners-guide-to-recurrent-networks-and-lstms)\n",
    "- [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [This tutorial provides a great introduction to building a simple LSTM for timeseries forecasting.](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) which \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-the-unemployment-data-for-timeseries-modeling\"></a>\n",
    "## Prepare the unemployment data for timeseries modeling\n",
    "---\n",
    "\n",
    "First we will load quarterly US unemployment data to do some basic forecasting using LSTMs. \n",
    "\n",
    "**Load the unemployment data from CSV file and perform any required cleaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['year_quarter', 'unemployment_rate']\n",
    "data['unemployment_rate'] = data['unemployment_rate'].map(lambda x: float(str(x).replace('%','')))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year_quarter'] = data['year_quarter'].apply(lambda x: x.replace('Q1','-03-31')\n",
    "                                                             .replace('Q2','-06-30')\n",
    "                                                             .replace('Q3','-09-30')\n",
    "                                                             .replace('Q4','-12-31'))\n",
    "\n",
    "data['year_quarter'] = data['year_quarter'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('year_quarter',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemployment_rate'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemployment_rate'].head(4*5).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "res = seasonal_decompose(data['unemployment_rate'])\n",
    "res.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-the-first-order-differenced-unemployment-rate\"></a>\n",
    "### Create the first-order differenced unemployment rate\n",
    "\n",
    "In timeseries modeling the raw timeseries is rarely used. Typically we will use the first order differenced timeseries (or further order differences). Technically this differencing is done to ensure that the timeseries is stationary. \n",
    "\n",
    "However, there are more intuitive reasons why we would want to model the differences as opposed to the actual values. Take the AAPL stock price, for example, and imagine we are a day-trader. If we hold the AAPL stock, we are of course interested in having a model that can predict the price of the stock in the future. However, what we are *really* interested in is how the stock price will *change* in the future from the current point. It is more useful to say \"the stock will go up by one point\" than it is to say \"the stock price will be 51.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['udiff'] = data['unemployment_rate'].diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize-the-differenced-unemployement-data-with-minmaxscaler\"></a>\n",
    "### Normalize the differenced unemployement data with `MinMaxScaler`\n",
    "\n",
    "We want the rate to be restricted to the range -1 to 1.\n",
    "\n",
    "> **Note:** the differencing will make the first value of the series a NaN value. Make sure to drop this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urate = data[['udiff']]\n",
    "mms = MinMaxScaler(feature_range=(-1, 1))\n",
    "urate = mms.fit_transform(urate)\n",
    "print(urate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['udiff'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-the-timeseries-into--traintest-splits\"></a>\n",
    "### Split the timeseries into 50% train/test splits\n",
    "\n",
    "We don't want a random train/test split in this case. With timeseries data, we are interested in how are model generalizes to future data in particular. Make the test set the second half of the data through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(urate) * 0.50)\n",
    "test_size = len(urate) - train_size\n",
    "print(len(urate), train_size, test_size)\n",
    "\n",
    "train, test = urate[0:train_size,:], urate[train_size:len(urate),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"write-a-function-to-create-the-predictor-and-target-data\"></a>\n",
    "## Write a function to create the predictor and target data\n",
    "---\n",
    "\n",
    "The function will need to create a Y target and X predictor. The X predictor matrix will simply be the shifted versions of Y, our target unemployment timeseries. In other words, we want our features to be previous timesteps of the target data for given lags.\n",
    "\n",
    "**Make a function with two arguments:**\n",
    "1. The timeseries data.\n",
    "2. The number of lags of the timeseries to have as predictors.\n",
    "\n",
    "The default should create a dataset where X is the unemployment rate a given time (t) and Y is the unemployment rate at the next time (t + 1). At the default lag of 1 X will just be the unemployment timeseries of Y shifted back by 1.\n",
    "\n",
    "> **Note:** make sure that the output X and Y are of the same length! You will need to slice off a row (at least - depends on the lag order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udiff = data.udiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udiff = udiff[0:8]\n",
    "udiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict(y=udiff.shift(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(timeseries, lag=1, as_array=True):\n",
    "    if not isinstance(timeseries, pd.Series):\n",
    "        timeseries = pd.Series(timeseries.ravel())\n",
    "    y = timeseries[lag:]\n",
    "    X = pd.DataFrame({'lag'+str(lag-i):timeseries.shift(-i) for i in range(0, lag)}).dropna().iloc[:-1, :]\n",
    "    if not as_array:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data(udiff, lag=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-training-and-testing-data-for-a-lag-of-\"></a>\n",
    "### Create training and testing data for a lag of 1\n",
    "\n",
    "Again, this means our X will just have 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 1\n",
    "trainX, trainY = create_data(train, lag)\n",
    "testX, testY = create_data(test, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reshape-the-data-to-work-with-the-lstm\"></a>\n",
    "## Reshape the data to work with the LSTM\n",
    "---\n",
    "\n",
    "The format of data the LSTM expects is:\n",
    "\n",
    "    [samples, time_steps, features]\n",
    "    \n",
    "Which is a 3D matrix.\n",
    "\n",
    "We have been using 2D predictor matrices for our machine learning algorithms, where our X predictor matrix has been in the form:\n",
    "\n",
    "    [samples, features]\n",
    "\n",
    "Since we are working with timeseries (which is the data an LSTM expects), we are now required to provide information about the time.\n",
    "\n",
    "You can use the `np.reshape()` command to turn your 2D X matrix into a 3D matrix that works for the LSTM matrix. We will talk about the \"time step\" dimension more down the line. \n",
    "\n",
    "> **Note:** In the case of a single lag this time step dimension is redundant. Later on, when we redesign the X matrix so that our individual features have multiple timesteps, this 3D format requirement will be clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"constructing-the-keras-model\"></a>\n",
    "## Constructing the Keras model\n",
    "\n",
    "Our LSTM model will be constructed in three parts.\n",
    "\n",
    "First initialize the sequential layer-to-layer neural network model:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "    \n",
    "Add an LSTM layer with 4 blocks/cells/neurons. We specify the `input_shape` to be the same dimensions as our features. You will notice that the `input_shape` below takes a tuple `(None, lag)`. The `None` is a placeholder for the timesteps of our features. By putting `None` we are simply avoiding specifying the timesteps.\n",
    "\n",
    "```python\n",
    "model.add(LSTM(4, input_shape=(None, lag)))\n",
    "```\n",
    "\n",
    "Add the output layer as a layer of one neuron that is fully connected to all of the previous LSTM cells:\n",
    "\n",
    "```python\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(None, lag)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-the-lstm-model\"></a>\n",
    "### Fit the LSTM model\n",
    "\n",
    "We can fit the model with these commands:\n",
    "\n",
    "```python\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "```\n",
    "\n",
    "Which will use the squared error loss (regression) and fit the data over 100 \"epochs\", or passes over the training data. It makes multiple passes because these LSTM neural networks learn according to a learning rate (which we have not specified).\n",
    "\n",
    "The `optimizer='adam'` selects the type of algorithm for gradient descent. The Adam optimizer performs well and is often recommended for many types of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot-the-original-data-the-training-predictions-and-the-testing-predictions\"></a>\n",
    "### Plot the original data, the training predictions and the testing predictions\n",
    "\n",
    "You can predict from a Keras model much like with a sklearn model:\n",
    "\n",
    "```python\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "```\n",
    "\n",
    "Make sure you lag the data forward for training and testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(urate)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(urate)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[-len(testPredict):, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(urate)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lstm-with-multiple-lags-as-predictors\"></a>\n",
    "## LSTM with multiple lags as predictors\n",
    "---\n",
    "\n",
    "We can instead predict the unemployment rate from not just the rate prior, but the `t-1`, `t-2`, and `t-3` rates.\n",
    "\n",
    "You can use the function you wrote above to construct a new X and y where X now has 3 predictors according to the different lags.\n",
    "\n",
    "Create the new Y and X variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 3\n",
    "trainX, trainY = create_data(train, lag)\n",
    "testX, testY = create_data(test, lag)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "print(trainX[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(None, lag)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(urate)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(urate)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[-len(testPredict):, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(urate)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refraing-the-problem-using-the-lstm-time-steps-dimension\"></a>\n",
    "## Refraing the problem using the LSTM \"time steps\" dimension\n",
    "---\n",
    "\n",
    "Recall that our X matrix is converted to the form:\n",
    "\n",
    "    [samples, time steps, features]\n",
    "    \n",
    "In the model we just made, we were saying that we had 3 different features, each of 1 time step long. This works fine, but it would be more appropriate to say that we had 1 feature with three different time steps, since that's what the data actually is (the unemplyment rate lagged to different degrees).\n",
    "\n",
    "Instead of reshaping our data where the time step dimension is 1, we can instead reshape it so that the feature dimension is 1 and the time step dimension is 3, like so:\n",
    "\n",
    "```python\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "```\n",
    "\n",
    "In this toy example, this will for all intents and purposes be the same, but it is more appropriate to specify it this way since the variable is the same.\n",
    "\n",
    "Recreate your training and testing data but reshaping your lags on the time dimension rather than the feature dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 3\n",
    "trainX, trainY = create_data(train, lag)\n",
    "testX, testY = create_data(test, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild-and-fit-the-lstm-model-and-plot-the-predictions\"></a>\n",
    "### Rebuild and fit the LSTM model, and plot the predictions\n",
    "\n",
    "You will need to change the `input_shape` now to be `input_shape=(lag, 1)`, which indicates we have \"lag\" number of timesteps and 1 predictor/feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(lag, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(urate)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(urate)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[-len(testPredict):, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(urate)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stateful-lstm-models\"></a>\n",
    "## \"Stateful\" LSTM models\n",
    "---\n",
    "\n",
    "There is another even more appropriate way we can construct this LSTM model. Right now we are restricting the model to use the previous 3 timesteps only, but wouldn't it be better to allow the model to \"remember\" *all* of the previous timesteps? How would we do this?\n",
    "\n",
    "LSTM models in Keras can be set as \"stateful\". This means that instead of resetting their internal state after each training batch, the internal state of the neurons is maintained. We can use this to our advantage to have the LSTM maintain a memory of all the previous timesteps as it fits:\n",
    "1. Build the X and y data with lag 1.\n",
    "2. Reshape the data to be 3D.\n",
    "3. Set the batch size to 1 (this means that only 1 observation will be fed into the model at a time.)\n",
    "4. Construct the LSTM model with a stateful LSTM layer.\n",
    "5. Fit the model multiple times setting `nb_ephoch=1`, `shuffle=False`, and `batch_size=1`. This will go through the observations sequentially till the end, feeding them into the model one by one.\n",
    "6. At the end of each iteration through all the data, call `model.reset_states()` to reset the internal state manually for the next iteration through.\n",
    "\n",
    "The code to do this is outlined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 1\n",
    "trainX, trainY = create_data(train, lag)\n",
    "testX, testY = create_data(test, lag)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "# You will now give it a \"batch_input_shape\" and specify the batch size\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, lag, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "# You need to reset the states manually for prediction with a stateful LSTM!!\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(urate)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
    " \n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(urate)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[-len(testPredict):, :] = testPredict\n",
    " \n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(urate)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prediction with LSTMs\n",
    "\n",
    "Adapted from https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char in list(char2idx)[:20]:\n",
    "    print(f'  {repr(char):4}: {char2idx[char]:3},')\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(''.join(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                    batch_input_shape=[BATCH_SIZE, None]))\n",
    "model.add(tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "  \n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "  \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "  \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
